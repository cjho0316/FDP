[2024-03-20 08:32:46,022] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: elt_reddit_pipeline.upload_to_s3 scheduled__2024-03-19T00:00:00+00:00 [queued]>
[2024-03-20 08:32:46,028] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: elt_reddit_pipeline.upload_to_s3 scheduled__2024-03-19T00:00:00+00:00 [queued]>
[2024-03-20 08:32:46,028] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-03-20 08:32:46,028] {taskinstance.py:1357} INFO - Starting attempt 2 of 2
[2024-03-20 08:32:46,029] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-03-20 08:32:46,040] {taskinstance.py:1377} INFO - Executing <Task(BashOperator): upload_to_s3> on 2024-03-19 00:00:00+00:00
[2024-03-20 08:32:46,045] {standard_task_runner.py:52} INFO - Started process 336 to run task
[2024-03-20 08:32:46,048] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'elt_reddit_pipeline', 'upload_to_s3', 'scheduled__2024-03-19T00:00:00+00:00', '--job-id', '5', '--raw', '--subdir', 'DAGS_FOLDER/elt_reddit_pipeline.py', '--cfg-path', '/tmp/tmpzwmabnn6', '--error-file', '/tmp/tmpv8c5q7u6']
[2024-03-20 08:32:46,049] {standard_task_runner.py:80} INFO - Job 5: Subtask upload_to_s3
[2024-03-20 08:32:46,088] {task_command.py:370} INFO - Running <TaskInstance: elt_reddit_pipeline.upload_to_s3 scheduled__2024-03-19T00:00:00+00:00 [running]> on host 9aae7e0ec976
[2024-03-20 08:32:46,132] {taskinstance.py:1571} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=elt_reddit_pipeline
AIRFLOW_CTX_TASK_ID=upload_to_s3
AIRFLOW_CTX_EXECUTION_DATE=2024-03-19T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=2
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-03-19T00:00:00+00:00
[2024-03-20 08:32:46,133] {subprocess.py:62} INFO - Tmp dir root location: 
 /tmp
[2024-03-20 08:32:46,134] {subprocess.py:74} INFO - Running command: ['bash', '-c', 'python /opt/***/extraction/upload_aws_s3_etl.py 20240320']
[2024-03-20 08:32:46,142] {subprocess.py:85} INFO - Output:
[2024-03-20 08:32:46,530] {subprocess.py:92} INFO - Traceback (most recent call last):
[2024-03-20 08:32:46,530] {subprocess.py:92} INFO -   File "/opt/***/extraction/upload_aws_s3_etl.py", line 73, in <module>
[2024-03-20 08:32:46,531] {subprocess.py:92} INFO -     main()
[2024-03-20 08:32:46,531] {subprocess.py:92} INFO -   File "/opt/***/extraction/upload_aws_s3_etl.py", line 35, in main
[2024-03-20 08:32:46,531] {subprocess.py:92} INFO -     create_bucket_if_not_exists(conn)
[2024-03-20 08:32:46,531] {subprocess.py:92} INFO -   File "/opt/***/extraction/upload_aws_s3_etl.py", line 53, in create_bucket_if_not_exists
[2024-03-20 08:32:46,531] {subprocess.py:92} INFO -     conn.meta.client.head_bucket(Bucket=BUCKET_NAME)
[2024-03-20 08:32:46,531] {subprocess.py:92} INFO -   File "/home/***/.local/lib/python3.7/site-packages/botocore/client.py", line 508, in _api_call
[2024-03-20 08:32:46,532] {subprocess.py:92} INFO -     return self._make_api_call(operation_name, kwargs)
[2024-03-20 08:32:46,532] {subprocess.py:92} INFO -   File "/home/***/.local/lib/python3.7/site-packages/botocore/client.py", line 875, in _make_api_call
[2024-03-20 08:32:46,532] {subprocess.py:92} INFO -     api_params, operation_model, context=request_context
[2024-03-20 08:32:46,532] {subprocess.py:92} INFO -   File "/home/***/.local/lib/python3.7/site-packages/botocore/client.py", line 933, in _convert_to_request_dict
[2024-03-20 08:32:46,532] {subprocess.py:92} INFO -     api_params, operation_model, context
[2024-03-20 08:32:46,532] {subprocess.py:92} INFO -   File "/home/***/.local/lib/python3.7/site-packages/botocore/client.py", line 969, in _emit_api_params
[2024-03-20 08:32:46,532] {subprocess.py:92} INFO -     context=context,
[2024-03-20 08:32:46,532] {subprocess.py:92} INFO -   File "/home/***/.local/lib/python3.7/site-packages/botocore/hooks.py", line 412, in emit
[2024-03-20 08:32:46,532] {subprocess.py:92} INFO -     return self._emitter.emit(aliased_event_name, **kwargs)
[2024-03-20 08:32:46,532] {subprocess.py:92} INFO -   File "/home/***/.local/lib/python3.7/site-packages/botocore/hooks.py", line 256, in emit
[2024-03-20 08:32:46,533] {subprocess.py:92} INFO -     return self._emit(event_name, kwargs)
[2024-03-20 08:32:46,533] {subprocess.py:92} INFO -   File "/home/***/.local/lib/python3.7/site-packages/botocore/hooks.py", line 239, in _emit
[2024-03-20 08:32:46,533] {subprocess.py:92} INFO -     response = handler(**kwargs)
[2024-03-20 08:32:46,533] {subprocess.py:92} INFO -   File "/home/***/.local/lib/python3.7/site-packages/botocore/handlers.py", line 276, in validate_bucket_name
[2024-03-20 08:32:46,533] {subprocess.py:92} INFO -     raise ParamValidationError(report=error_msg)
[2024-03-20 08:32:46,533] {subprocess.py:92} INFO - botocore.exceptions.ParamValidationError: Parameter validation failed:
[2024-03-20 08:32:46,533] {subprocess.py:92} INFO - Invalid bucket name ""janghodashboardbucket"": Bucket name must match the regex "^[a-zA-Z0-9.\-_]{1,255}$" or be an ARN matching the regex "^arn:(aws).*:(s3|s3-object-lambda):[a-z\-0-9]*:[0-9]{12}:accesspoint[/:][a-zA-Z0-9\-.]{1,63}$|^arn:(aws).*:s3-outposts:[a-z\-0-9]+:[0-9]{12}:outpost[/:][a-zA-Z0-9\-]{1,63}[/:]accesspoint[/:][a-zA-Z0-9\-]{1,63}$"
[2024-03-20 08:32:46,578] {subprocess.py:96} INFO - Command exited with return code 1
[2024-03-20 08:32:46,601] {taskinstance.py:1889} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/bash.py", line 195, in execute
    f'Bash command failed. The command returned a non-zero exit code {result.exit_code}.'
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2024-03-20 08:32:46,603] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=elt_reddit_pipeline, task_id=upload_to_s3, execution_date=20240319T000000, start_date=20240320T083246, end_date=20240320T083246
[2024-03-20 08:32:46,611] {standard_task_runner.py:97} ERROR - Failed to execute job 5 for task upload_to_s3 (Bash command failed. The command returned a non-zero exit code 1.; 336)
[2024-03-20 08:32:46,639] {local_task_job.py:156} INFO - Task exited with return code 1
[2024-03-20 08:32:46,665] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
